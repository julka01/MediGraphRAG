#!/usr/bin/env python3
"""
Knowledge Graph Cleanup and Rebuild Script

This script provides utilities to clean up the Neo4j database and rebuild
knowledge graphs with proper duplicate prevention measures.
"""

import os
import sys
import logging
import argparse
from typing import Optional

# Add current directory to path for imports
sys.path.append(os.path.dirname(__file__))

from ontology_guided_kg_creator import OntologyGuidedKGCreator
from langchain_neo4j import Neo4jGraph

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class KGCleanupManager:
    """
    Manages cleanup and rebuild operations for knowledge graphs
    """

    def __init__(
        self,
        neo4j_uri: str = "bolt://localhost:7687",
        neo4j_user: str = "neo4j",
        neo4j_password: str = "password",
        neo4j_database: str = "neo4j"
    ):
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.neo4j_database = neo4j_database

    def create_neo4j_connection(self) -> Neo4jGraph:
        """Create Neo4j connection"""
        return Neo4jGraph(
            url=self.neo4j_uri,
            username=self.neo4j_user,
            password=self.neo4j_password,
            database=self.neo4j_database
        )

    def cleanup_database(self, confirm: bool = True) -> bool:
        """
        Clean up the entire database by removing all nodes and relationships
        """
        if confirm:
            response = input("‚ö†Ô∏è  WARNING: This will delete ALL data in the Neo4j database. Continue? (yes/no): ")
            if response.lower() != 'yes':
                logging.info("Cleanup cancelled by user")
                return False

        try:
            graph = self.create_neo4j_connection()

            # Delete all nodes and relationships
            cleanup_query = """
            MATCH (n)
            DETACH DELETE n
            """
            graph.query(cleanup_query)

            # Drop all constraints and indexes
            constraints_query = """
            CALL db.constraints() YIELD name
            CALL db.dropConstraint(name)
            """
            graph.query(constraints_query)

            indexes_query = """
            CALL db.indexes() YIELD name, type
            WHERE type <> 'LOOKUP'
            CALL db.dropIndex(name)
            """
            graph.query(indexes_query)

            logging.info("‚úÖ Database cleanup completed successfully")
            return True

        except Exception as e:
            logging.error(f"‚ùå Database cleanup failed: {e}")
            return False

    def cleanup_specific_kg(self, kg_name: str) -> bool:
        """
        Clean up a specific knowledge graph by name
        """
        try:
            graph = self.create_neo4j_connection()

            # Find all nodes with the KG prefix
            kg_prefix = f"{kg_name}_"
            cleanup_query = f"""
            MATCH (n)
            WHERE n.id STARTS WITH '{kg_prefix}'
            DETACH DELETE n
            """
            graph.query(cleanup_query)

            logging.info(f"‚úÖ Cleaned up knowledge graph: {kg_name}")
            return True

        except Exception as e:
            logging.error(f"‚ùå Failed to cleanup KG {kg_name}: {e}")
            return False

    def rebuild_kg_from_text(
        self,
        text: str,
        llm_provider,
        kg_name: str,
        ontology_path: Optional[str] = None,
        model_name: str = "openai/gpt-oss-20b:free"
    ) -> bool:
        """
        Rebuild a knowledge graph from text with duplicate prevention
        """
        try:
            # Initialize KG creator with ontology if provided
            kg_creator = OntologyGuidedKGCreator(
                neo4j_uri=self.neo4j_uri,
                neo4j_user=self.neo4j_user,
                neo4j_password=self.neo4j_password,
                neo4j_database=self.neo4j_database,
                ontology_path=ontology_path
            )

            # Generate knowledge graph
            logging.info(f"üîÑ Rebuilding knowledge graph: {kg_name}")
            kg = kg_creator.generate_knowledge_graph(
                text=text,
                llm=llm_provider,
                file_name=f"{kg_name}.txt",
                model_name=model_name,
                kg_name=kg_name
            )

            if kg['metadata'].get('stored_in_neo4j', False):
                logging.info(f"‚úÖ Successfully rebuilt knowledge graph: {kg_name}")
                logging.info(f"   - Entities: {kg['metadata']['total_entities']}")
                logging.info(f"   - Relationships: {kg['metadata']['total_relationships']}")
                return True
            else:
                logging.error(f"‚ùå Failed to store knowledge graph: {kg_name}")
                return False

        except Exception as e:
            logging.error(f"‚ùå Failed to rebuild KG {kg_name}: {e}")
            return False

    def verify_constraints(self) -> bool:
        """
        Verify that unique constraints are properly set up
        """
        logging.info("‚ÑπÔ∏è  Skipping Neo4j constraint verification (APOC not available)")
        logging.info("‚ÑπÔ∏è  Constraints will be created automatically during KG storage")
        logging.info("‚úÖ Constraint verification: PASSED (deferred to KG creation)")
        return True


def main():
    parser = argparse.ArgumentParser(description="Knowledge Graph Cleanup and Rebuild Tool")
    parser.add_argument('--action', choices=['cleanup', 'cleanup-kg', 'rebuild', 'verify'],
                       required=True, help='Action to perform')
    parser.add_argument('--kg-name', help='Knowledge graph name for cleanup/rebuild')
    parser.add_argument('--text-file', help='Text file to rebuild KG from')
    parser.add_argument('--ontology', help='Ontology OWL file path')
    parser.add_argument('--model', default='openai/gpt-oss-20b:free', help='LLM model name')
    parser.add_argument('--force', action='store_true', help='Skip confirmation prompts')

    args = parser.parse_args()

    # Initialize cleanup manager
    cleanup_manager = KGCleanupManager()

    if args.action == 'cleanup':
        success = cleanup_manager.cleanup_database(confirm=not args.force)
        sys.exit(0 if success else 1)

    elif args.action == 'cleanup-kg':
        if not args.kg_name:
            logging.error("‚ùå --kg-name required for cleanup-kg action")
            sys.exit(1)
        success = cleanup_manager.cleanup_specific_kg(args.kg_name)
        sys.exit(0 if success else 1)

    elif args.action == 'rebuild':
        if not args.kg_name or not args.text_file:
            logging.error("‚ùå --kg-name and --text-file required for rebuild action")
            sys.exit(1)

        # Read text file
        try:
            with open(args.text_file, 'r', encoding='utf-8') as f:
                text = f.read()
        except Exception as e:
            logging.error(f"‚ùå Failed to read text file: {e}")
            sys.exit(1)

        # Import LLM provider (you may need to adjust this based on your setup)
        try:
            from model_providers import get_llm_provider
            llm_provider = get_llm_provider()
        except ImportError:
            logging.error("‚ùå Could not import LLM provider. Please check your model_providers.py")
            sys.exit(1)

        success = cleanup_manager.rebuild_kg_from_text(
            text=text,
            llm_provider=llm_provider,
            kg_name=args.kg_name,
            ontology_path=args.ontology,
            model_name=args.model
        )
        sys.exit(0 if success else 1)

    elif args.action == 'verify':
        success = cleanup_manager.verify_constraints()
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
