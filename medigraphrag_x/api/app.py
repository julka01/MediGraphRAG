from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Body
from fastapi.responses import JSONResponse, FileResponse
from fastapi.encoders import jsonable_encoder
from fastapi.staticfiles import StaticFiles
from typing import Optional
import os, uuid, sys, tempfile, io
from dotenv import load_dotenv

load_dotenv()

from model_providers import get_provider as get_llm_provider
from enhanced_kg_creator import UnifiedOntologyGuidedKGCreator
from csv_processor import MedicalReportCSVProcessor

# Import from local kg_utils
from kg_utils.extract_graph import extract_graph_from_file_local_file
from kg_utils.graph_query import get_graphDB_driver

# Retain actual langchain_experimental if available
# import importlib
# if "langchain_experimental" not in sys.modules:
#     importlib.import_module("langchain_experimental")

# Core graph imports moved inside endpoint functions

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

# Global storage for current graph data
current_graph_data = None

@app.on_event("startup")
def check_neo4j_connection():
    try:
        driver = get_graphDB_driver(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
        )
        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            session.run("RETURN 1").single()
        print("‚úÖ Neo4j connection check passed")
    except Exception as e:
        import sys
        print(f"‚ö†Ô∏è Neo4j health check failed: {e}", file=sys.stderr)
        print("‚ö†Ô∏è Continuing without Neo4j connection - some features may not work", file=sys.stderr)
        # Don't raise error - allow app to start

@app.get("/health/neo4j")
def neo4j_health():
    try:
        driver = get_graphDB_driver(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
        )
        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            record = session.run("RETURN count(*) AS c").single()
        return {"status": "ok", "nodeCount": record["c"]}
    except Exception as e:
        from fastapi import HTTPException, status
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Neo4j health check failed: {e}"
        )

@app.get("/")
async def root():
    return FileResponse("static/index.html")

@app.post("/load_kg_from_file")
async def load_kg_from_file(
    file: UploadFile = File(...),
    provider: str = Form("openai"),
    model: str = Form("gpt-3.5-turbo")
):
    """
    Return the full raw KG from llm-graph-builder (no ontology filtering).
    """
    try:
        data = await file.read()
        tmp_dir = tempfile.gettempdir()
        file_path = os.path.join(tmp_dir, file.filename)
        with open(file_path, "wb") as tmpf:
            tmpf.write(data)
        latency, graph_data = await extract_graph_from_file_local_file(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
            model,
            file_path,
            file.filename,
            [], [], 1000, 100, 1,
            None, None
        )
        driver = get_graphDB_driver(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
        )
        nodes = []
        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            for record in session.run("MATCH (n) RETURN n"):
                node = record["n"]
                props = {}
                for k, v in dict(node).items():
                    if hasattr(v, "isoformat"):
                        props[k] = v.isoformat()
                    else:
                        props[k] = v
                nodes.append({"id": node.id, "labels": list(node.labels), "properties": props})
        relationships = []
        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            for record in session.run("MATCH ()-[r]->() RETURN r"):
                rel = record["r"]
                props = {}
                for k, v in dict(rel).items():
                    if hasattr(v, "isoformat"):
                        props[k] = v.isoformat()
                    else:
                        props[k] = v
                relationships.append({
                    "id": rel.id,
                    "type": rel.type,
                    "start": rel.start_node.id,
                    "end": rel.end_node.id,
                    "properties": props
                })
        return JSONResponse(content=jsonable_encoder({"kg_id": str(uuid.uuid4()), "graph_data": {"nodes": nodes, "relationships": relationships}}))
    except (ImportError, ModuleNotFoundError) as e:
        return JSONResponse(content=jsonable_encoder({"kg_id": str(uuid.uuid4()), "graph_data": {"nodes": [], "relationships": []}}))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/extract_graph")
async def extract_graph(
    file: UploadFile = File(...),
    provider: str = Form("openai"),
    model: str = Form("gpt-3.5-turbo")
):
    """
    Return raw graph JSON without ontology post-processing.
    """
    try:
        data = await file.read()
        tmp_dir = tempfile.gettempdir()
        file_path = os.path.join(tmp_dir, file.filename)
        with open(file_path, "wb") as tmpf:
            tmpf.write(data)
        _, graph_data = await extract_graph_from_file_local_file(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
            model,
            file_path,
            file.filename,
            [], [], 1000, 100, 1,
            None, None
        )
        return JSONResponse(content=graph_data)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create_ontology_guided_kg")
async def create_ontology_guided_kg(
    file: UploadFile = File(...),
    provider: str = Form("openai"),
    model: str = Form("gpt-3.5-turbo"),
    embedding_model: str = Form("sentence_transformers"),
    ontology_file: Optional[UploadFile] = File(None),
    max_chunks: int = Form(None),
    kg_name: str = Form(None),
    neo4j_uri: str = Form(None),
    neo4j_user: str = Form(None),
    neo4j_password: str = Form(None),
    neo4j_database: str = Form(None)
):
    """
    Create knowledge graph with optional ontology guidance.
    If ontology file is provided, uses it to ensure consistent entity types and relationships.
    If no ontology is provided, performs basic LLM-based entity extraction.
    """
    try:
        # Read file content with proper encoding handling
        data = await file.read()

        # Determine file type and extract text appropriately
        file_extension = os.path.splitext(file.filename)[1].lower()

        import fitz  # PyMuPDF

        if file_extension == '.pdf':
            # Extract text from PDF using PyMuPDF (fitz) like test
            try:
                doc = fitz.open(stream=io.BytesIO(data), filetype="pdf")
                text_content = ""
                for page in doc:
                    text_content += page.get_text() + "\n"
                doc.close()
                if len(text_content.strip()) == 0:
                    raise HTTPException(status_code=400, detail="PDF file contains no extractable text")
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"PDF processing failed: {str(e)}")
        else:
            # Try to decode as UTF-8 text file
            try:
                text_content = data.decode('utf-8')
            except UnicodeDecodeError:
                # Fallback: try latin-1 or ignore errors
                text_content = data.decode('latin-1', errors='ignore')

        if len(text_content.strip()) == 0:
            raise HTTPException(status_code=400, detail="File contains no readable text content")

        print(f"üéØ Creating KG with model: {model} from provider: {provider}")
        print(f"üìÑ File type: {file_extension}, Size: {len(data)} bytes, Text length: {len(text_content)} chars")

        # Get LLM provider (use defaults matching test if not specified)
        provider = provider or "openrouter"
        model = model or "openai/gpt-oss-20b:free"
        llm = get_llm_provider(provider, model)

        # DEBUG: Log ontology file information early
        print(f"üîç Ontology file parameter: ontology_file = {ontology_file}")
        if ontology_file:
            print(f"üîç Ontology file details: filename={ontology_file.filename}, content_type={getattr(ontology_file, 'content_type', 'unknown')}")

        # Handle ontology file if provided
        ontology_path = None
        if ontology_file:
            # Save ontology file temporarily
            ontology_data = await ontology_file.read()
            tmp_dir = tempfile.gettempdir()
            ontology_filename = f"ontology_{uuid.uuid4()}{os.path.splitext(ontology_file.filename)[1]}"
            ontology_path = os.path.join(tmp_dir, ontology_filename)
            with open(ontology_path, "wb") as tmpf:
                tmpf.write(ontology_data)
            print(f"üìö Using provided ontology: {ontology_file.filename} -> {ontology_path}")
            print(f"üß™ Ontology file size: {len(ontology_data)} bytes")

            # Enhanced verification with file content check
            if os.path.exists(ontology_path):
                file_size = os.path.getsize(ontology_path)
                print(f"‚úÖ Ontology file saved successfully: {file_size} bytes")

                # Try to read a small portion to verify it's valid
                try:
                    with open(ontology_path, 'r', encoding='utf-8') as f:
                        sample_content = f.read(500)
                        if 'owl:' in sample_content.lower() or 'rdf:' in sample_content.lower():
                            print(f"‚úÖ Ontology file appears to be valid OWL/RDF format")
                        else:
                            print(f"‚ö†Ô∏è Ontology file may not be valid OWL/RDF format (no ontology tags found in first 500 chars)")
                            print(f"üìÑ Sample content: {sample_content[:200]}...")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not read ontology file content: {e}")
            else:
                print(f"‚ùå Ontology file not found after saving: {ontology_path}")

                # Log temp directory contents for debugging
                try:
                    temp_files = os.listdir(tmp_dir)
                    ontology_files = [f for f in temp_files if f.startswith('ontology_')]
                    print(f"üìÅ Temp directory contents: {len(temp_files)} files, {len(ontology_files)} ontology files")
                except Exception as e:
                    print(f"‚ùå Could not list temp directory: {e}")

        else:
            print(f"‚ÑπÔ∏è No ontology file provided in request")

        # Generate unique KG name if not provided
        if not kg_name:
            kg_name = f"kg_{str(uuid.uuid4())}"

        # DEBUG: Log ontology_path being passed to KG creator
        print(f"üîÑ Initializing OntologyGuidedKGCreator with ontology_path: {ontology_path}")

        # Initialize ontology-guided KG creator (with defaults matching test)
        # Use provided neo4j credentials or fall back to environment variables
        neo4j_uri = neo4j_uri or os.getenv("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = neo4j_user or os.getenv("NEO4J_USERNAME", "neo4j")
        neo4j_password = neo4j_password or os.getenv("NEO4J_PASSWORD", "password")
        neo4j_database = neo4j_database or os.getenv("NEO4J_DATABASE", "neo4j")

        kg_creator = UnifiedOntologyGuidedKGCreator(
            chunk_size=2000,  # Larger chunks for better patient report context
            chunk_overlap=300,
            ontology_path=ontology_path,
            neo4j_uri=neo4j_uri,
            neo4j_user=neo4j_user,
            neo4j_password=neo4j_password,
            neo4j_database=neo4j_database,
            embedding_model=embedding_model or "sentence_transformers"
        )

        # DEBUG: Verify ontology state after initialization
        print(f"üîç After KG creator initialization:")
        print(f"   - kg_creator.ontology_path: {kg_creator.ontology_path}")
        print(f"   - kg_creator.ontology_classes: {len(kg_creator.ontology_classes)} loaded")
        print(f"   - kg_creator.ontology_relationships: {len(kg_creator.ontology_relationships)} loaded")
        if kg_creator.ontology_classes:
            print(f"   - Sample classes: {kg_creator.ontology_classes[:3]}")
        if kg_creator.ontology_relationships:
            print(f"   - Sample relationships: {kg_creator.ontology_relationships[:3]}")

        # Generate KG with ontology guidance (or without if no ontology provided)
        kg = kg_creator.generate_knowledge_graph(text_content, llm, file.filename, model, max_chunks, kg_name)

        # Log results like test script
        entities = kg.get('metadata', {}).get('total_entities', 0)
        relationships = kg.get('metadata', {}).get('total_relationships', 0)
        stored = kg.get('metadata', {}).get('stored_in_neo4j', False)

        print(f"üìä Initial KG Results:")
        print(f"   - Entities: {entities}")
        print(f"   - Relationships: {relationships}")
        print(f"   - Stored in Neo4j: {stored}")

        # Reload KG from Neo4j to ensure ontology labels are properly displayed
        loaded_kg = None
        if stored:
            print("üîÑ Reloading KG from Neo4j to ensure proper ontology labels...")
            from kg_loader import KGLoader

            kg_loader = KGLoader()
            reload_success = False
            if kg_name:
                # Load by KG name (ontology label) - now includes Document nodes
                loaded_kg = kg_loader.load_from_neo4j(
                    uri=neo4j_uri,
                    user=neo4j_user,
                    password=neo4j_password,
                    kg_label=kg_name
                )
                if loaded_kg and loaded_kg.get('status') == 'success':
                    reload_success = True
            else:
                # Load all entities (excluding system nodes)
                loaded_kg = kg_loader.load_from_neo4j(
                    uri=neo4j_uri,
                    user=neo4j_user,
                    password=neo4j_password
                )
                if loaded_kg and loaded_kg.get('status') == 'success':
                    reload_success = True

            if reload_success:
                loaded_entities = loaded_kg.get('loaded_nodes', 0) if 'loaded_nodes' in loaded_kg else len(loaded_kg.get('nodes', []))
                loaded_relationships = loaded_kg.get('loaded_relationships', 0) if 'loaded_relationships' in loaded_kg else len(loaded_kg.get('relationships', []))
                print(f"‚úÖ Reloaded {loaded_entities} nodes, {loaded_relationships} relationships from Neo4j")
            else:
                print("‚ö†Ô∏è Failed to reload KG from Neo4j, using initial KG data")
                loaded_kg = None

        # Use reloaded KG data if available and valid, otherwise use initial KG
        final_kg_data = loaded_kg if loaded_kg and loaded_kg.get('status') == 'success' else kg

        method = "ontology_guided" if ontology_path else "basic_llm"
        determinism_improvements = [
            "fixed_chunk_size",
            "temperature=0_for_all_LLMs",
            "node_label_fix",
            "neo4j_reload" if loaded_kg else None
        ]
        determinism_improvements = [x for x in determinism_improvements if x is not None]
        if ontology_path:
            determinism_improvements.append("ontology_constraints_applied")

        return JSONResponse(content={
            "kg_id": str(uuid.uuid4()),
            "graph_data": final_kg_data,
            "method": method,
            "ontology_file": ontology_file.filename if ontology_file else None,
            "determinism_improvements": determinism_improvements
        })

    except HTTPException:
        raise
    except Exception as e:
        # Provide fallback: try to return the initial KG data even if Neo4j operations failed
        error_msg = f"Ontology-guided KG creation failed: {str(e)}"

        # If we have initial KG data (created before Neo4j failure), return it
        if 'kg' in locals() and kg:
            print(f"‚ö†Ô∏è Neo4j storage/reload failed, but returning locally generated KG data")
            return JSONResponse(content={
                "kg_id": str(uuid.uuid4()),
                "graph_data": kg,
                "method": "ontology_guided" if ontology_path else "basic_llm",
                "ontology_file": ontology_file.filename if ontology_file else None,
                "determinism_improvements": [
                    "fixed_chunk_size",
                    "temperature=0_for_all_LLMs",
                    "node_label_fix"
                ] + (["ontology_constraints_applied"] if ontology_path else []),
                "warning": "Neo4j connection/storage failed - using locally processed data only",
                "error_details": error_msg
            })

        # No fallback data available, raise the error
        raise HTTPException(status_code=500, detail=error_msg)

@app.post("/chat")
async def chat(body: dict = Body(...)):
    """
    Enhanced KG-focused RAG chat that ensures responses come from KG alone.
    """
    try:
        question = body.get("question")
        docs = body.get("document_names", [])
        session = body.get("session_id", "default_session")
        mode = body.get("mode", "default")
        provider = body.get("provider_rag", "openrouter")
        model = body.get("model_rag", "openai/gpt-oss-20b:free")

        if not question:
            raise HTTPException(status_code=422, detail="Missing question")

        # Use the EnhancedRAGSystem for strict KG-only responses
        from enhanced_rag_system import EnhancedRAGSystem
        from model_providers import LangChainRunnableAdapter

        # Create RAG system with direct KG connection
        rag_system = EnhancedRAGSystem()

        # Get LLM provider
        llm = LangChainRunnableAdapter(get_llm_provider(provider, model), model)

        # Generate response using KG data only
        result = rag_system.generate_response(question, llm, docs)

        # Format response to match expected structure
        if "error" in result:
            return JSONResponse(content={
                "session_id": session,
                "message": f"KG Error: {result['error']}",
                "info": {
                    "sources": [],
                    "model": model,
                    "nodedetails": [],
                    "total_tokens": 0,
                    "response_time": 0,
                    "mode": mode,
                    "entities": result.get("entities", []),
                    "metric_details": {},
                    "kg_only": True,
                    "kg_stats": result.get("context", {}).get("kg_stats", {})
                },
                "user": "chatbot"
            })

        # Convert enhanced response to standard format
        return JSONResponse(content={
            "session_id": session,
            "message": result["response"],
            "info": {
                "sources": result["sources"],
                "model": model,
                "nodedetails": {
                    "chunkdetails": result.get("context", {}).get("chunks", []),
                    "entitydetails": result.get("context", {}).get("entities", {}),
                    "communitydetails": []
                },
                "total_tokens": 0,  # TODO: Implement token counting
                "response_time": 0,
                "mode": mode,
                "entities": {
                    "entityids": result.get("entities", []),
                    "relationshipids": [r.get("key", "") for r in result.get("relationships", [])],
                    "used_entities": result.get("used_entities", [])  # Nodes highlighted in KG visualization
                },
                "metric_details": {
                    "question": question,
                    "contexts": [chunk["text"] for chunk in result.get("context", {}).get("chunks", [])],
                    "answer": result["response"]
                },
                "kg_only": True,
                "chunk_count": result.get("chunk_count", 0),
                "entity_count": result.get("entity_count", 0),
                "relationship_count": result.get("relationship_count", 0),
                "confidence": result.get("confidence", 0.0)
            },
            "user": "chatbot"
        })

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"KG-RAG Error: {str(e)}")

@app.get("/models/{provider}")
def list_models(provider: str):
    """
    Return available models for provider.
    """
    model_map = {
        "openai": ["gpt-4", "gpt-3.5-turbo"],
        "openrouter": ["openai/gpt-oss-20b:free", "meta-llama/llama-3.3-8b-instruct:free", "deepseek/deepseek-chat-v3.1:free", "x-ai/grok-4-fast:free"]
    }
    return {"models": model_map.get(provider.lower(), [])}

@app.get("/neo4j/default_credentials")
def default_credentials():
    """
    Return Neo4j credentials for frontend.
    """
    return {
        "uri": os.getenv("NEO4J_URI"),
        "user": os.getenv("NEO4J_USERNAME"),
        "password": os.getenv("NEO4J_PASSWORD"),
        "database": os.getenv("NEO4J_DATABASE")
    }

@app.post("/clear_kg")
async def clear_kg():
    """
    Clear the entire Neo4j knowledge graph by removing all nodes and relationships.
    """
    try:
        # Use Neo4j driver from environment
        driver = get_graphDB_driver(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
        )

        print('üßπ Clearing entire Neo4j knowledge graph...')

        with driver.session() as session:
            # First, disable constraints that might interfere with deletion
            try:
                constraints_result = session.run("SHOW CONSTRAINTS")
                constraints = [record["name"] for record in constraints_result]
                for constraint in constraints:
                    try:
                        session.run(f"DROP CONSTRAINT {constraint}")
                        print(f'‚úÖ Dropped constraint: {constraint}')
                    except Exception as e:
                        print(f'‚ö†Ô∏è Could not drop constraint {constraint}: {e}')
            except Exception as e:
                print(f'‚ö†Ô∏è Error getting constraints: {e}')

            # Drop indexes that might interfere
            try:
                indexes_result = session.run("SHOW INDEXES")
                indexes = [record["name"] for record in indexes_result if record["type"] != "LOOKUP"]
                for index in indexes:
                    try:
                        session.run(f"DROP INDEX {index}")
                        print(f'‚úÖ Dropped index: {index}')
                    except Exception as e:
                        print(f'‚ö†Ô∏è Could not drop index {index}: {e}')
            except Exception as e:
                print(f'‚ö†Ô∏è Error getting indexes: {e}')

            # Delete all relationships first
            session.run("MATCH ()-[r]-() DELETE r")
            print("‚úÖ Deleted all relationships")

            # Delete all nodes
            result = session.run("MATCH (n) DELETE n RETURN count(n) as deleted_count")
            record = result.single()
            deleted_count = record["deleted_count"] if record else 0
            print(f"‚úÖ Deleted all {deleted_count} nodes")

            # Attempt garbage collection (APOC procedure - may not be available)
            try:
                session.run("CALL db.resample.index.all()")
                print("‚úÖ Index resampled successfully")
            except Exception as e:
                print(f"‚ö†Ô∏è Index resampling failed (APOC not available): {e}")
                print("‚ö†Ô∏è Continuing without index resampling - this is usually fine")

        print('üéâ Successfully cleared the entire Neo4j knowledge graph!')

        return JSONResponse(content={
            "message": f"Knowledge graph cleared successfully! Deleted {deleted_count} nodes and all relationships.",
            "status": "cleared",
            "nodes_deleted": deleted_count
        })

    except Exception as e:
        import traceback
        print(f'‚ùå Error clearing KG: {e}')
        print(f'Traceback: {traceback.format_exc()}')
        raise HTTPException(status_code=500, detail=f"Failed to clear knowledge graph: {str(e)}")

@app.post("/test_create_working_kg")
async def test_create_working_kg():
    """
    Create a working test KG with vector embeddings that the RAG can actually use with semantic similarity search.
    """
    try:
        # Use the get_graphDB_driver function for consistency
        driver = get_graphDB_driver(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USERNAME"),
            os.getenv("NEO4J_PASSWORD"),
            os.getenv("NEO4J_DATABASE"),
        )

        print('Connected to Neo4j, creating test KG with placeholder embeddings...')

        # Clear any existing test data and indexes
        with driver.session() as session:
            try:
                print('üîÑ Clearing existing test data...')

                # First drop existing vector indexes
                try:
                    session.run('DROP INDEX vector IF EXISTS')
                    session.run('DROP INDEX entity_vector IF EXISTS')
                    print('‚úÖ Dropped existing vector indexes')
                except Exception as e:
                    print(f'Index drop warning (may not exist): {e}')

                # Clear all test data with broader criteria
                session.run('MATCH (n) WHERE n.id STARTS WITH "test_" OR n.fileName = "test_medical_data.txt" DETACH DELETE n')
                session.run('MATCH (n) WHERE n.id IS NOT NULL AND n.id =~ "^\d+$" DETACH DELETE n')  # Clear numeric IDs from failed runs
                session.run('MATCH (n) WHERE n.id IS NOT NULL AND toString(n.id) =~ "^\d+$" DETACH DELETE n')  # Clear string numeric IDs

                # Clean up all related data step by step
                session.run('MATCH ()-[r]-() DELETE r')  # Delete all relationships first
                session.run('MATCH (n) WHERE n.fileName = "test_medical_data.txt" DETACH DELETE n')
                session.run('MATCH (n:Document) DETACH DELETE n')
                session.run('MATCH (n:Chunk) DETACH DELETE n')
                session.run('MATCH (n:__Entity__) DETACH DELETE n')
                session.run('MATCH (n) WHERE labels(n)[] IS NULL DETACH DELETE n')  # Clean up orphaned nodes

                print('‚úÖ Cleared existing test data')
            except Exception as e:
                print(f'Cleanup warning: {e}')

            # Create sample data with UUIDs
            doc_id = str(uuid.uuid4())
            chunk_id = str(uuid.uuid4())

            test_text = """Prostate cancer is a disease that affects men. Common symptoms include frequent urination, difficulty urinating, blood in urine, and erectile dysfunction. Treatment options include surgery (radical prostatectomy), radiation therapy, hormone therapy, and active surveillance for low-risk cases."""

            # Create document
            session.run('MERGE (d:Document {fileName: "test_medical_data.txt"}) SET d.id = $doc_id', {'doc_id': doc_id})

            # Create chunk with simple embeddings (placeholder for now - we can upgrade to real embeddings later)
            chunk_embedding = [0.1] * 384  # Placeholder vector similar to all-MiniLM-L6-v2 dimension

            session.run('''
            MERGE (c:Chunk {id: $chunk_id})
            SET c.text = $text,
                c.embedding = $embedding,
                c.position = 0
            ''', {
                'chunk_id': chunk_id,
                'text': test_text,
                'embedding': chunk_embedding
            })

            # Link chunk to document
            session.run('MATCH (c:Chunk {id: $chunk_id}), (d:Document {id: $doc_id}) MERGE (c)-[:PART_OF]->(d)', {
                'chunk_id': chunk_id,
                'doc_id': doc_id
            })

            # No test entities created - removed hardcoded test entities

        print('‚úÖ Test KG with embeddings created successfully!')

        # Test the KG by querying it
        with driver.session() as session:
            stats = session.run('''
            MATCH (d:Document) WHERE d.fileName = "test_medical_data.txt"
            OPTIONAL MATCH (d)<-[:PART_OF]-(c:Chunk)
            OPTIONAL MATCH (c)-[:HAS_ENTITY]->(e:__Entity__)
            OPTIONAL MATCH (e)-[r]-()
            RETURN count(DISTINCT d) AS docs, count(DISTINCT c) AS chunks, count(DISTINCT e) AS entities, count(DISTINCT r) AS rels
            ''')

            result = stats.peek() if stats.peek() is not None else None

        return JSONResponse(content={
            "message": "Test KG with vector embeddings created successfully",
            "status": "ready_with_embeddings",
            "embeddings": "enabled",
            "vector_indexes": "created",
            "test_queries": [
                "What are the symptoms of prostate cancer?",
                "What treatments are available for prostate cancer?",
                "How does prostate cancer affect men?"
            ],
            "data_stats": {
                "documents": result.get('docs', 1) if 'result' in locals() and result else 1,
                "chunks": result.get('chunks', 1) if 'result' in locals() and result else 1,
                "entities": 0,  # No test entities created
                "relationships": 0,  # No test relationships created
                "embedding_dimensions": 384,
                "similarity_function": "cosine"
            }
        })

    except Exception as e:
        import traceback
        print(f'‚ùå Error creating test KG: {e}')
        print(f'Traceback: {traceback.format_exc()}')
        raise HTTPException(status_code=500, detail=f"Test KG creation failed: {str(e)}")


@app.post("/save_kg_to_neo4j")
async def save_kg_to_neo4j(
    kg_id: str = Form(...),
    uri: str = Form(...),
    user: str = Form(...),
    password: str = Form(...)
):
    """
    Save knowledge graph data to Neo4j database.
    """
    try:
        global current_graph_data

        # Check if we have graph data to save
        if current_graph_data is None:
            raise HTTPException(status_code=400, detail="No graph data available to save. Load a KG first.")

        driver = get_graphDB_driver(uri, user, password, os.getenv("NEO4J_DATABASE"))

        nodes_saved = 0
        relationships_saved = 0

        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            # Clear existing data first (optional - you might want to keep this or make it configurable)
            try:
                session.run("MATCH (n) DETACH DELETE n")
                print("Cleared existing Neo4j database")
            except Exception as e:
                print(f"Warning: Could not clear existing data: {e}")

            # Save nodes
            for node in current_graph_data.get('nodes', []):
                labels = node.get('labels', [])
                if not labels:
                    labels = ['Entity']  # Default label

                # Build Cypher MERGE query
                label_str = ':'.join(f'`{label}`' for label in labels)
                properties = {}

                # Copy node properties, excluding internal ones
                for k, v in node.get('properties', {}).items():
                    if k not in ['embedding', 'element_id'] and v is not None:
                        properties[k] = v

                # Add id if it exists
                if node.get('properties', {}).get('id'):
                    properties['id'] = node['properties']['id']
                elif node.get('id'):
                    properties['id'] = str(node['id'])

                # Build parameterized MERGE query
                prop_str = ', '.join(f'`{k}`: ${k}' for k, v in properties.items())
                if prop_str:
                    merge_query = f"MERGE (n:{label_str} {{ {prop_str} }})"
                else:
                    # Fallback for nodes with no properties - use id if available
                    node_id = properties.get('id', str(node.get('id', nodes_saved)))
                    merge_query = f"MERGE (n:{label_str} {{ id: '{node_id}' }})"

                try:
                    param_dict = {k: v for k, v in properties.items()}
                    session.run(merge_query, param_dict)
                    nodes_saved += 1
                except Exception as e:
                    print(f"Error saving node {node.get('id', 'unknown')}: {e}")
                    continue

            # Save relationships
            for rel in current_graph_data.get('relationships', []):
                start_id = rel.get('start') or rel.get('from')
                end_id = rel.get('end') or rel.get('to')
                rel_type = rel.get('type', 'RELATED_TO')

                if not start_id or not end_id:
                    continue

                # Build relationship query
                properties = {}
                for k, v in rel.get('properties', {}).items():
                    if v is not None:
                        properties[k] = v

                prop_str = ', '.join(f'`{k}`: ${k}' for k, v in properties.items())
                rel_prop = f" {{{prop_str}}}" if prop_str else ""

                match_query = f"""
                MATCH (a), (b)
                WHERE id(a) = {start_id} AND id(b) = {end_id}
                MERGE (a)-[r:`{rel_type}`{rel_prop}]->(b)
                """

                try:
                    param_dict = {k: v for k, v in properties.items()}
                    session.run(match_query, param_dict)
                    relationships_saved += 1
                except Exception as e:
                    print(f"Error saving relationship {start_id}-{rel_type}->{end_id}: {e}")
                    continue

        return JSONResponse(content={
            "message": "Knowledge graph saved to Neo4j successfully",
            "kg_id": kg_id,
            "nodes_saved": nodes_saved,
            "relationships_saved": relationships_saved
        })

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save KG to Neo4j: {str(e)}")

@app.post("/load_kg_from_neo4j")
async def load_kg_from_neo4j(
    uri: str = Form(...),
    user: str = Form(...),
    password: str = Form(...),
    limit: int = Form(1000),
    sample_mode: bool = Form(False),
    load_complete: bool = Form(False),
    kg_label: str = Form(None)
):
    """
    Load the entire KG from Neo4j with optional sampling and filtering.
    """
    try:
        driver = get_graphDB_driver(uri, user, password, os.getenv("NEO4J_DATABASE"))
        with driver.session(database=os.getenv("NEO4J_DATABASE")) as session:
            total_nodes = session.run("MATCH (n) RETURN count(n) AS c").single()["c"]
            total_rels = session.run("MATCH ()-[r]->() RETURN count(r) AS c").single()["c"]
            # Determine node query
            if load_complete:
                node_query = "MATCH (n) RETURN n"
            elif sample_mode:
                node_query = f"MATCH (n) RETURN n ORDER BY rand() LIMIT {limit}"
            else:
                node_query = f"MATCH (n) RETURN n LIMIT {limit}"
            # Apply label filter if provided
            if kg_label:
                node_query = node_query.replace("MATCH (n)", f"MATCH (n:{kg_label})")

            # Process nodes with proper handling for DateTime objects
            nodes = []
            for record in session.run(node_query):
                node = record["n"]
                props = {}
                for k, v in dict(node).items():
                    if hasattr(v, "isoformat"):  # Handle DateTime and other temporal objects
                        props[k] = v.isoformat()
                    else:
                        props[k] = v
                nodes.append({"id": node.id, "labels": list(node.labels), "properties": props})

            # Process relationships with proper handling for DateTime objects
            relationships = []
            for record in session.run("MATCH (n)-[r]->(m) RETURN r"):
                rel = record["r"]
                props = {}
                for k, v in dict(rel).items():
                    if hasattr(v, "isoformat"):  # Handle DateTime and other temporal objects
                        props[k] = v.isoformat()
                    else:
                        props[k] = v
                relationships.append({
                    "id": rel.id,
                    "type": rel.type,
                    "start": rel.start_node.id,
                    "end": rel.end_node.id,
                    "properties": props
                })

        stats = {
            "total_nodes_in_db": total_nodes,
            "total_relationships_in_db": total_rels,
            "loaded_nodes": len(nodes),
            "loaded_relationships": len(relationships),
            "sample_mode": sample_mode,
            "complete_import": load_complete
        }
        return JSONResponse(content={
            "kg_id": str(uuid.uuid4()),
            "graph_data": {"nodes": nodes, "relationships": relationships},
            "stats": stats
        })
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/validate_csv")
async def validate_csv(csv_file: UploadFile = File(...)):
    """
    Validate CSV file format and structure for medical reports.
    """
    try:
        print(f"üîç Validating CSV file: {csv_file.filename}")

        # Save uploaded file temporarily
        data = await csv_file.read()
        tmp_dir = tempfile.gettempdir()
        csv_path = os.path.join(tmp_dir, f"validate_{uuid.uuid4()}.csv")

        with open(csv_path, "wb") as tmpf:
            tmpf.write(data)

        # Initialize CSV processor
        processor = MedicalReportCSVProcessor(delimiter='|')

        # Validate format
        validation_result = processor.validate_csv_format(csv_path)

        # Clean up temp file
        try:
            os.unlink(csv_path)
        except Exception as e:
            print(f"Warning: Could not delete temp file {csv_path}: {e}")

        return JSONResponse(content={
            "is_valid": validation_result.get("is_valid", False),
            "delimiter": validation_result.get("delimiter", "|"),
            "num_columns": validation_result.get("num_columns", 0),
            "num_rows": validation_result.get("num_rows", 0),
            "field_mappings_count": len(validation_result.get("field_mappings", {})),
            "columns": validation_result.get("columns", []),
            "field_mappings": validation_result.get("field_mappings", {}),
            "errors": validation_result.get("validation_errors", [])
        })

    except Exception as e:
        import traceback
        print(f"‚ùå CSV validation error: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"CSV validation failed: {str(e)}")

@app.post("/bulk_process_csv")
async def bulk_process_csv(
    csv_file: UploadFile = File(...),
    batch_size: int = Form(50, description="Number of reports to process per batch"),
    start_row: int = Form(0, description="Starting row number (0-based)"),
    max_chunks: int = Form(20, description="Maximum number of chunks to process per report (for testing)")
):
    """
    Process multiple medical reports from CSV in bulk batches.
    """
    try:
        print(f"üîÑ Starting bulk CSV processing: {csv_file.filename}")
        print(f"   Batch size: {batch_size}, Start row: {start_row}")

        # Save uploaded file temporarily
        data = await csv_file.read()
        tmp_dir = tempfile.gettempdir()
        csv_path = os.path.join(tmp_dir, f"bulk_{uuid.uuid4()}.csv")

        with open(csv_path, "wb") as tmpf:
            tmpf.write(data)

        # Initialize enhanced KG creator for bulk processing
        provider = "openrouter"
        model = "openai/gpt-oss-20b:free"
        llm = get_llm_provider(provider, model)

        kg_creator = UnifiedOntologyGuidedKGCreator(
            chunk_size=2000,
            chunk_overlap=300,
            neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            neo4j_user=os.getenv("NEO4J_USERNAME", "neo4j"),
            neo4j_password=os.getenv("NEO4J_PASSWORD", "password"),
            neo4j_database=os.getenv("NEO4J_DATABASE", "neo4j"),
            embedding_model="sentence_transformers",
            max_chunks=max_chunks
        )

        # Process CSV in bulk
        bulk_result = kg_creator.bulk_process_medical_reports(
            csv_path=csv_path,
            start_row=start_row,
            batch_size=batch_size
        )

        # Clean up temp file
        try:
            os.unlink(csv_path)
        except Exception as e:
            print(f"Warning: Could not delete temp file {csv_path}: {e}")

        kg_id = str(uuid.uuid4())  # Generate a unique KG ID for this bulk processing session

        return JSONResponse(content={
            "kg_id": kg_id,
            "message": f"Successfully processed {bulk_result.get('total_reports_processed', 0)} medical reports from CSV",
            "total_reports_processed": bulk_result.get("total_reports_processed", 0),
            "total_kgs": bulk_result.get("total_knowledge_graphs", 0),
            "total_entities": bulk_result.get("total_entities", 0),
            "total_relationships": bulk_result.get("total_relationships", 0),
            "batch_size": batch_size,
            "start_row": start_row,
            "processing_details": bulk_result.get("processing_details", []),
            "csv_validation": bulk_result.get("csv_validation", {}),
            "bulk_processing_info": {
                "batch_size_used": batch_size,
                "start_row": start_row,
                "timestamp": str(uuid.uuid4())[:8]  # Simple timestamp
            }
        })

    except Exception as e:
        import traceback
        print(f"‚ùå Bulk CSV processing error: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Bulk CSV processing failed: {str(e)}")

@app.get("/static/medical_reports_template.csv")
async def serve_csv_template():
    """
    Serve the medical reports CSV template for download.
    """
    try:
        processor = MedicalReportCSVProcessor()

        # Create template in memory
        tmp_dir = tempfile.gettempdir()
        template_path = os.path.join(tmp_dir, f"template_{uuid.uuid4()}.csv")

        processor.create_csv_template(template_path, num_sample_rows=3)

        # Read and return file
        def cleanup_temp_file():
            try:
                os.unlink(template_path)
            except Exception as e:
                print(f"Warning: Could not delete temp template file: {e}")

        from starlette.responses import StreamingResponse
        import io

        with open(template_path, 'rb') as f:
            content = f.read()

        # Schedule cleanup (non-blocking)
        import threading
        threading.Timer(10.0, cleanup_temp_file).start()  # Delete after 10 seconds

        headers = {
            'Content-Disposition': 'attachment; filename="medical_reports_template.csv"',
            'Content-Type': 'text/csv'
        }

        return StreamingResponse(io.BytesIO(content), headers=headers)

    except Exception as e:
        print(f"‚ùå Error serving CSV template: {e}")
        raise HTTPException(status_code=500, detail=f"Could not generate CSV template: {str(e)}")
